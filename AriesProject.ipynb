{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1 : Setting Up Enviornment"
      ],
      "metadata": {
        "id": "WG8Wz_vDPcZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!git clone https://github.com/parth1620/Project-NST.git"
      ],
      "metadata": {
        "id": "in1BRY63KQB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBRqlDttzhmO"
      },
      "source": [
        "## Task 2 : Loading VGG Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_route = \"path/to/content_image\"\n",
        "style_route = \"path/to/style_image\""
      ],
      "metadata": {
        "id": "J5-U37eKP9_J"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlfO4Dn4zhmP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "vgg=models.vgg19(pretrained=True)\n",
        "print(vgg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "VQduWdGnzhmS"
      },
      "outputs": [],
      "source": [
        "vgg=vgg.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Rg0tgeEozhmW"
      },
      "outputs": [],
      "source": [
        "for parameters in vgg.parameters():\n",
        "  parameters.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX91rnk_zhma"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unl3h53-gM3C"
      },
      "outputs": [],
      "source": [
        "vgg.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxM2YyaVzhmd"
      },
      "source": [
        "## Task 3 : Preprocess image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRJZpXiAxb9l"
      },
      "source": [
        "Torchvision models page : https://pytorch.org/docs/stable/torchvision/models.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "40te2Uhkjg1o"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "a7szQ02nzhme"
      },
      "outputs": [],
      "source": [
        "def preprocess(img_path,max_size=500):\n",
        "  image = Image.open(img_path).convert(\"RGB\")\n",
        "  if max(image.size)>max_size:\n",
        "    size=max_size\n",
        "  else:\n",
        "    size=image.size\n",
        "  img_transforms = T.Compose([\n",
        "      T.Resize(size),\n",
        "      T.ToTensor(),\n",
        "      T.Normalize(mean=[0.485,0.456,0.406],\n",
        "                  std=[0.229,0.224,0.225])\n",
        "      ])\n",
        "\n",
        "  image = img_transforms(image)\n",
        "  image = image.unsqueeze(0)\n",
        "  return image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osMP6zX4zhmh"
      },
      "outputs": [],
      "source": [
        "content_p=preprocess(content_route)\n",
        "content_s=preprocess(style_route)\n",
        "\n",
        "content_p=content_p.to(device)\n",
        "style_p=content_s.to(device)\n",
        "print(content_p.shape)\n",
        "print(content_s.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylW3-U354e8k"
      },
      "source": [
        "## Task 4 : Deprocess image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "oHOZgcQnzhmk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def deprocess(tensor):\n",
        "  image= tensor.to('cpu').clone()\n",
        "  image = image.numpy()\n",
        "  image=image.squeeze(0)\n",
        "  image=image.transpose(1,2,0)\n",
        "  image = image*np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406])\n",
        "  image= image.clip(0,1)\n",
        "  return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uleFVOuyzhmo"
      },
      "outputs": [],
      "source": [
        "content_d=deprocess(content_p)\n",
        "style_d=deprocess(content_s)\n",
        "\n",
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\n",
        "ax1.imshow(content_d)\n",
        "ax2.imshow(style_d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vohyJ0Eizhmu"
      },
      "source": [
        "## Task 5 : Get content,style features and create gram matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "48e6BNHGzhmv"
      },
      "outputs": [],
      "source": [
        "def get_features(image,model):\n",
        "  layers={\n",
        "      '0': 'conv1_1',\n",
        "      '5': 'conv2_1',\n",
        "      '10': 'conv3_1',\n",
        "      '19': 'conv4_1',\n",
        "      '21': 'conv4_2',\n",
        "      '28': 'conv5_1'\n",
        "  }\n",
        "\n",
        "  x=image\n",
        "  features={}\n",
        "  for name,layer in model._modules.items():\n",
        "    x=layer(x)\n",
        "    if name in layers:\n",
        "      features[layers[name]]=x\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "c1PhbsKizhmy"
      },
      "outputs": [],
      "source": [
        "content_f=get_features(content_p,vgg)\n",
        "style_f=get_features(style_p,vgg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "qxp17pv5zhm2"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "  b,c,h,w=tensor.size()\n",
        "  tensor = tensor.view(c,h*w)\n",
        "  gram = torch.mm(tensor,tensor.t())\n",
        "  return gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "vSrDvJsTVuyg"
      },
      "outputs": [],
      "source": [
        "style_grams={layer: gram_matrix(style_f[layer]) for layer in style_f}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYG8nmYnzhm6"
      },
      "source": [
        "## Task 6 : Creating Style and Content loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "D8bZVJGGzhm6"
      },
      "outputs": [],
      "source": [
        "def content_loss(content,target):\n",
        "  loss=torch.mean((content-target)**2)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "b4dUvmSwzhm-"
      },
      "outputs": [],
      "source": [
        "style_weights={\n",
        "    'conv1_1': 0.75,\n",
        "    'conv2_1': 0.5,\n",
        "    'conv3_1': 0.1,\n",
        "    'conv4_1': 0.1,\n",
        "    'conv5_1': 0.05,\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "4RYWY5EnzhnB"
      },
      "outputs": [],
      "source": [
        "def style_loss(style_weights,target_features,style_grams):\n",
        "  loss=0\n",
        "  for layer in style_weights:\n",
        "    target_f = target_features[layer]\n",
        "    target_gram = gram_matrix(target_f)\n",
        "    style_gram=style_grams[layer]\n",
        "    b,c,h,w=target_f.shape\n",
        "    layer_loss=style_weights[layer]*torch.mean((style_gram-target_gram)**2)\n",
        "    loss+=layer_loss\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiZqKKUmZ-d-"
      },
      "outputs": [],
      "source": [
        "target=content_p.clone().requires_grad_(True).to(device)\n",
        "target_f=get_features(target,vgg)\n",
        "print(content_loss(target_f['conv4_2'],content_f['conv4_2']))\n",
        "print(style_loss(style_weights,target_f,style_grams))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHF2xVr1zhnE"
      },
      "source": [
        "## Task 7 : Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "OalABhfjzhnF"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "optimizer=optim.Adam([target],lr=0.002)\n",
        "alpha=1\n",
        "beta=4\n",
        "tv_weight=1e-6\n",
        "\n",
        "epochs=3501\n",
        "show_every=500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cDFL71FQzhnI"
      },
      "outputs": [],
      "source": [
        "def total_loss(c_loss, s_loss, tv_loss, alpha, beta, tv_weight):\n",
        "    loss = alpha * c_loss + beta * s_loss + tv_weight * tv_loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def total_variation_loss(image):\n",
        "    tv_loss = torch.sum(torch.abs(image[:, :, :-1] - image[:, :, 1:])) + torch.sum(torch.abs(image[:, :-1, :] - image[:, 1:, :]))\n",
        "    return tv_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmjKkgBhzhnO"
      },
      "outputs": [],
      "source": [
        "result = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    target_f = get_features(target, vgg)\n",
        "    c_loss = content_loss(target_f['conv4_2'], content_f['conv4_2'])\n",
        "    s_loss = style_loss(style_weights, target_f, style_grams)\n",
        "    tv_loss = total_variation_loss(target)\n",
        "    loss = total_loss(c_loss, s_loss, tv_loss, alpha, beta, tv_weight)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % show_every == 0:\n",
        "        print(f\"Loss after epoch {i}: {loss.item()}\")\n",
        "        result.append(deprocess(target.detach()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGzdEJt_UAr9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(len(result)):\n",
        "    plt.subplot(4, 2, i + 1)\n",
        "    plt.imshow(result[i])\n",
        "plt.show()\n",
        "\n",
        "print(\"Final results are:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 8 : Final Result"
      ],
      "metadata": {
        "id": "Z1-FumIHY_7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(content_d)\n",
        "plt.title('Initial Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(result[-1])\n",
        "plt.title('Final Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1GJViN23SXtG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}